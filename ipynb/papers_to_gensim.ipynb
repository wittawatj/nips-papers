{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Word2Vec in `gensim` to train a word embedding model using the content from NIPS papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#config InlineBackend.figure_format = 'pdf'\n",
    "from IPython.core.display import HTML\n",
    "import gensim as gen\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "try:\n",
    "    import cPickle as pickle \n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "import scipy.sparse as sp\n",
    "import string\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the pickle containing the document-term matrix, \n",
    "# put the abstracts in, and dump it to a file.\n",
    "fyear = 1988\n",
    "tyear = 2015\n",
    "dt_fpath = 'DT_%d_%d_wabs.p'%(fyear, tyear)\n",
    "\n",
    "with open(dt_fpath, 'r') as f:\n",
    "    info = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info.keys()\n",
    "list_abs = info['abstracts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_abs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make each abstract a list of words\n",
    "list_list_abs = [ab.split(' ') for ab in list_abs if ab is not None]\n",
    "print list_list_abs[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gensim word2vec\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html#id6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paper_dataframe(fpath):\n",
    "    rows = []\n",
    "    with open(fpath, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        # Each read gives ['Id', 'Title', 'EventType', 'PdfName', 'Abstract', 'PaperText']\n",
    "        reader.next()\n",
    "        for row in reader:\n",
    "            rows.append(tuple(row))\n",
    "    data = pd.DataFrame(rows, columns=['Id', 'Title', 'EventType', \n",
    "                                   'PdfName', 'Abstract', 'PaperText'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ',sdf,.-23\\][](s)'\n",
    "re.sub(r'([^\\w])+', ' ', text, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_simple(text):\n",
    "    # replace spaces with one space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.DOTALL)\n",
    "    # remove non-English words\n",
    "    text = re.sub(r'[^\\w]+', ' ', text, flags=re.DOTALL)\n",
    "    # naive tokenization\n",
    "    tokens = [w.lower().strip() for w in text.split(' ') if len(w) > 1]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe = paper_dataframe('Papers1988_2015.csv')\n",
    "n_docs = dframe.shape[0]\n",
    "tok_papers = []\n",
    "tok_abstracts = []\n",
    "for i in xrange(n_docs):\n",
    "    paper = dframe['PaperText'][i]\n",
    "    paper_tokens = tokenize_simple(paper)\n",
    "    tok_papers.append(paper_tokens)\n",
    "    \n",
    "    ab = list_abs[i]\n",
    "    if ab is None:\n",
    "        ab_tokens = []\n",
    "    else:\n",
    "        ab_tokens = tokenize_simple(ab)\n",
    "    tok_abstracts.append(ab_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size means the latent dimension\n",
    "# sentences = an iterable where each item is a list of words\n",
    "size = 50\n",
    "window = 5\n",
    "dest_fname = 'w2v_size%d_win%d.p'%(size, window)\n",
    "model = w2v.Word2Vec(tok_papers, size=size, window=window, min_count=5, workers=4)\n",
    "model.save(dest_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.similarity('neural', 'deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity('neural', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match('supervised unsupervised neuron reinforcement'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match('kernel gretton hsic mmd'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'kernel' in model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a representation of each paper\n",
    "\n",
    "The representation is simply a set of embedded words taken from the abstract and the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = info['titles']\n",
    "# each element is the representation of the paper. \n",
    "# This is a matrix with each row corresponding to the embedding\n",
    "# of a word in the abstract and the title.\n",
    "paper_reps = []\n",
    "for i in xrange(n_docs):\n",
    "    title_tokens = tokenize_simple(titles[i])\n",
    "    rep_words = tok_abstracts[i] + title_tokens\n",
    "    \n",
    "    # embed each word in rep_words (if in the vocabulary)\n",
    "    rep = []\n",
    "    for w in rep_words:\n",
    "        # only embed words that are in the vocabulary\n",
    "        if w in model.wv:\n",
    "            embed = model.wv[w]\n",
    "            rep.append(embed)\n",
    "    mat = np.vstack(rep)\n",
    "    paper_reps.append(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(paper_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the pickle with the paper representations\n",
    "dt_dest = 'DT_%d_%d_wembed.p'%(fyear, tyear)\n",
    "info['paper_reps'] = paper_reps\n",
    "with open(dt_dest, 'w') as f:\n",
    "    pickle.dump(info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved pickle and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('DT_%d_%d_wembed.p'%(fyear, tyear), 'r') as f:\n",
    "    info = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DT = info['DT']\n",
    "abstracts = info['abstracts']\n",
    "paper_reps = info['paper_reps']\n",
    "titles = info['titles']\n",
    "words = info['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter words by DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document frequency of each word\n",
    "n_docs = DT.shape[0]\n",
    "DF = np.array( (DT > 0).sum(0) )[0]\n",
    "df_lb = 7\n",
    "df_ub = int(0.15*n_docs)\n",
    "\n",
    "print('n = #docs: %d'%n_docs)\n",
    "print('original #words: %d'%len(words))\n",
    "print('#words with %d <= df: %d'% (df_lb, np.sum(DF>=df_lb) ) )\n",
    "print('#words with df <= %d: %d'% (df_ub, np.sum(DF<=df_ub) ) )\n",
    "df_I = np.logical_and(DF>=df_lb, DF<=df_ub)\n",
    "print('#words with %d <= df <= %d: %d'% \n",
    "      (df_lb, df_ub, np.sum( df_I) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_words = np.array(words)[df_I]\n",
    "print df_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out words\n",
    "fDT = DT[:, df_I]\n",
    "fwords = np.array(words)[df_I].tolist()\n",
    "\n",
    "info['DT'] = fDT\n",
    "info['words'] = fwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dffiltered_fname = 'DT_%d_%d_wem_df%d_%d.p'%(fyear, tyear, df_lb, df_ub)\n",
    "with open(dffiltered_fname, 'w') as f:\n",
    "    pickle.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
