{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#config InlineBackend.figure_format = 'pdf'\n",
    "from IPython.core.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "try:\n",
    "    import cPickle as pickle \n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "import scipy.sparse as sp\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def paper_dataframe(fpath):\n",
    "    rows = []\n",
    "    with open(fpath, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        # Each read gives ['Id', 'Title', 'EventType', 'PdfName', 'Abstract', 'PaperText']\n",
    "        reader.next()\n",
    "        for row in reader:\n",
    "            rows.append(tuple(row))\n",
    "    data = pd.DataFrame(rows, columns=['Id', 'Title', 'EventType', \n",
    "                                   'PdfName', 'Abstract', 'PaperText'])\n",
    "    return data\n",
    "\n",
    "def is_printable(s):\n",
    "    \"\"\"True if all characters are printable. Implicitly assume English.\"\"\"\n",
    "    return all_chars_in(s, string.printable)\n",
    "\n",
    "def all_chars_in(s, char_set):\n",
    "    for c in s:\n",
    "        if c not in char_set:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def some_chars_in(s, char_set):\n",
    "    for c in s:\n",
    "        if c in char_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def no_char_in(s, char_set):\n",
    "    for c in s:\n",
    "        if c in char_set:\n",
    "            return False \n",
    "    return True\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tok = WhitespaceTokenizer()\n",
    "    words = [w.strip() for w in tok.tokenize(text)]\n",
    "    return words\n",
    "\n",
    "def uncond_filter_words(list_words):\n",
    "    \"\"\"Unconditionally remove words according to some rules\"\"\"\n",
    "    L = list_words\n",
    "    filters = [\n",
    "        lambda w: no_char_in(w, string.punctuation),\n",
    "        lambda w: no_char_in(w, '0123456789'),\n",
    "        # start with alphabets\n",
    "        lambda w: re.match('[a-z].+', w.lower()) is not None,\n",
    "        lambda w: len(w) >= 3 and len(w) <= 20, \n",
    "         # only printable characters\n",
    "        lambda w: is_printable(w), \n",
    "         # remove words that do not have English alphabet\n",
    "        lambda w: some_chars_in(w.lower(), string.lowercase),\n",
    "    ]\n",
    "    for i, f in enumerate(filters):\n",
    "        L = filter(f, L)\n",
    "    return L\n",
    "\n",
    "def list_to_file(L, fpath):\n",
    "    with open(fpath, 'w') as f:\n",
    "        for w in L:\n",
    "            f.write('%s\\n'%w)\n",
    "\n",
    "def file_to_list(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        L = f.readlines()\n",
    "    L = [w.strip() for w in L]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stack_papers(years, dest_path):\n",
    "    content = ''\n",
    "    for i, y in enumerate(years):\n",
    "        with open('../output%d/Papers.csv'%y, 'r') as pfile:\n",
    "            if i>0:\n",
    "                # skip header\n",
    "                pfile.readline()\n",
    "            content = content + pfile.read()\n",
    "            \n",
    "    # write\n",
    "    with open(dest_path, 'w') as dfile:\n",
    "        dfile.write(content)\n",
    "\n",
    "def hist_words(list_words):\n",
    "    \"\"\"Return a map: word->count from the list of words.\"\"\"\n",
    "    m = {}\n",
    "    for i, w in enumerate(list_words):\n",
    "        m[w] = 1 if w not in m else m[w] + 1\n",
    "    return m\n",
    "\n",
    "\n",
    "class CacheStemmer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.cache = {}\n",
    "        \n",
    "    def stem(self, word):\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        else:\n",
    "            st = self.stemmer.stem(word)\n",
    "            self.cache[word] = st\n",
    "            return st\n",
    "        \n",
    "        \n",
    "def stem_words(words, stemmer=CacheStemmer() ):\n",
    "    stem_words = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            s = stemmer.stem(w)\n",
    "            stem_words.append(s)\n",
    "        except UnicodeDecodeError as e:\n",
    "            #print('decode error for: %s'%w)\n",
    "            pass\n",
    "    return stem_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fyear = 1988\n",
    "tyear = 2015\n",
    "stack_fname = 'Papers%d_%d.csv'%(fyear, tyear)\n",
    "stack_papers(range(fyear, tyear+1), stack_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Get the list of total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = CacheStemmer()\n",
    "set_words = set()\n",
    "paper_folders = ['../output%d'%y for y in range(fyear, tyear+1)]\n",
    "\n",
    "# dictionary compiled from Scowl spelling checking\n",
    "dict_all = file_to_list('dict_all.txt')\n",
    "set_stop = set(file_to_list('stop_words.txt'))\n",
    "set_dict = set(dict_all)\n",
    "for oi, out_fol in enumerate(paper_folders):\n",
    "    fpath = os.path.join(out_fol, 'Papers.csv')\n",
    "    data = paper_dataframe(fpath)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        abstract = data['Abstract'][i]\n",
    "        content = data['PaperText'][i]\n",
    "        title = data['Title'][i]\n",
    "\n",
    "        title_words = tokenize(title)\n",
    "        abs_words = tokenize(abstract)\n",
    "        content_words = tokenize(content)\n",
    "        words = title_words + abs_words + content_words\n",
    "        # only include words (before stemming) that are in the dictionary\n",
    "        # and not in the list of stop words\n",
    "        words = [w for w in words if w in set_dict and w not in set_stop]\n",
    "        stwords = stem_words(words, stemmer)\n",
    "        set_words.update(stwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncond_words = uncond_filter_words(set_words)\n",
    "uncond_words.sort()\n",
    "list_to_file(uncond_words, 'uncond_filtered_words.txt')\n",
    "print('unconditionally filters words: %d'%(len(uncond_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2ind = dict(zip(uncond_words, range(len(uncond_words))) )\n",
    "data = paper_dataframe(stack_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_ind = []\n",
    "word_ind = []\n",
    "counts = []\n",
    "titles = []\n",
    "for i in range(data.shape[0]):\n",
    "    abstract = data['Abstract'][i]\n",
    "    content = data['PaperText'][i]\n",
    "    title = data['Title'][i]\n",
    "    titles.append(title)\n",
    "\n",
    "    title_words = tokenize(title)\n",
    "    abs_words = tokenize(abstract)\n",
    "    content_words = tokenize(content)\n",
    "    di_words = title_words + abs_words + content_words\n",
    "    stwords = stem_words(di_words, stemmer)\n",
    "    hist = hist_words(stwords)\n",
    "    for w, c in hist.iteritems():\n",
    "        if w in w2ind:\n",
    "            # ignore terms which are not in the vocabulary\n",
    "            wi = w2ind[w]\n",
    "            doc_ind.append(i)\n",
    "            word_ind.append(wi)\n",
    "            counts.append(c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save DT to a file \n",
    "DT = sp.csr_matrix( (counts, (doc_ind, word_ind)), shape=(data.shape[0], len(uncond_words)) )\n",
    "dt_fpath = 'DT_%d_%d.p'%(fyear, tyear)\n",
    "info = {'DT': DT, 'words': uncond_words, 'titles': titles}\n",
    "with open(dt_fpath, 'w') as f:\n",
    "    pickle.dump(info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Filter DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(dt_fpath, 'r') as f:\n",
    "    info = pickle.load(f)\n",
    "    \n",
    "words = info['words']\n",
    "DT = info['DT']\n",
    "titles = info['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document frequency of each word\n",
    "n = DT.shape[0]\n",
    "DF = np.array( (DT > 0).sum(0) )[0]\n",
    "df_lb = 5\n",
    "df_ub = int(0.4*n)\n",
    "\n",
    "print('#docs: %d'%n)\n",
    "print('original #words: %d'%len(words))\n",
    "print('#words with %d <= df: %d'% (df_lb, np.sum(DF>=df_lb) ) )\n",
    "print('#words with df <= %d: %d'% (df_ub, np.sum(DF<=df_ub) ) )\n",
    "df_I = np.logical_and(DF>=df_lb, DF<=df_ub)\n",
    "print('#words with %d <= df <= %d: %d'% \n",
    "      (df_lb, df_ub, np.sum( df_I) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(DF))\n",
    "plt.xlabel('word index')\n",
    "plt.ylabel('doc frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_words = np.array(words)[df_I]\n",
    "df_words.tolist()\n",
    "list_to_file(df_words, 'words_df%d_%d.txt'%(df_lb, df_ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_some_substring(text, substrings):\n",
    "    \"\"\"True if the text contains at least one substring in the list.\"\"\"\n",
    "    for i, s in enumerate(substrings):\n",
    "        if text.lower().find(s) > -1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "stitles = sorted(titles)\n",
    "\n",
    "supervised_kws = ['large margin', 'classif', 'regression', 'kernel', 'ensemble', 'neural net']\n",
    "neuro_kws = ['motor control', 'neural', 'neuron', 'spiking', 'spike', 'cortex', 'plasticity', \n",
    "            'neural decod', 'neural encod', 'brain imag', 'biolog', 'perception', 'cognitive', \n",
    "            'emotion', 'synap', 'neural population', 'cortical', 'firing rate', 'firing-rate', ]\n",
    "bayesian_kws = ['graphical model', 'bayesian', 'inference', 'mcmc', 'monte carlo', \n",
    "               'posterior', 'prior', 'variational', 'markov', 'latent', 'probabilistic', \n",
    "               'exponential fami']\n",
    "kernel_kws = ['kernel', 'distribution embedding', 'support vector', 'gaussian process']\n",
    "neuro_titles = filter(lambda t: has_some_substring(t, neuro_kws), stitles)\n",
    "bayesian_titles = filter(lambda t: has_some_substring(t, bayesian_kws), stitles)\n",
    "supervised_titles = [t for t in stitles if has_some_substring(t, supervised_kws)]\n",
    "kernel_titles = [t for t in stitles if has_some_substring(t, kernel_kws)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bayesian_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(bayesian_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
