{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#config InlineBackend.figure_format = 'pdf'\n",
    "from IPython.core.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "try:\n",
    "    import cPickle as pickle \n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "import scipy.sparse as sp\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def paper_dataframe(fpath):\n",
    "    rows = []\n",
    "    with open(fpath, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        # Each read gives ['Id', 'Title', 'EventType', 'PdfName', 'Abstract', 'PaperText']\n",
    "        reader.next()\n",
    "        for row in reader:\n",
    "            rows.append(tuple(row))\n",
    "    data = pd.DataFrame(rows, columns=['Id', 'Title', 'EventType', \n",
    "                                   'PdfName', 'Abstract', 'PaperText'])\n",
    "    return data\n",
    "\n",
    "def is_printable(s):\n",
    "    \"\"\"True if all characters are printable. Implicitly assume English.\"\"\"\n",
    "    return all_chars_in(s, string.printable)\n",
    "\n",
    "def all_chars_in(s, char_set):\n",
    "    for c in s:\n",
    "        if c not in char_set:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def some_chars_in(s, char_set):\n",
    "    for c in s:\n",
    "        if c in char_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def no_char_in(s, char_set):\n",
    "    for c in s:\n",
    "        if c in char_set:\n",
    "            return False \n",
    "    return True\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tok = WhitespaceTokenizer()\n",
    "    words = [w.strip() for w in tok.tokenize(text)]\n",
    "    return words\n",
    "\n",
    "def uncond_filter_words(list_words):\n",
    "    \"\"\"Unconditionally remove words according to some rules\"\"\"\n",
    "    L = list_words\n",
    "    filters = [\n",
    "        lambda w: no_char_in(w, string.punctuation),\n",
    "        lambda w: no_char_in(w, '0123456789'),\n",
    "        # start with alphabets\n",
    "        lambda w: re.match('[a-z].+', w.lower()) is not None,\n",
    "        lambda w: len(w) >= 3 and len(w) <= 20, \n",
    "         # only printable characters\n",
    "        lambda w: is_printable(w), \n",
    "         # remove words that do not have English alphabet\n",
    "        lambda w: some_chars_in(w.lower(), string.lowercase),\n",
    "    ]\n",
    "    for i, f in enumerate(filters):\n",
    "        L = filter(f, L)\n",
    "    return L\n",
    "\n",
    "def list_to_file(L, fpath):\n",
    "    with open(fpath, 'w') as f:\n",
    "        for w in L:\n",
    "            f.write('%s\\n'%w)\n",
    "\n",
    "def file_to_list(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        L = f.readlines()\n",
    "    L = [w.strip() for w in L]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stack_papers(years, dest_path):\n",
    "    content = ''\n",
    "    for i, y in enumerate(years):\n",
    "        with open('../output%d/Papers.csv'%y, 'r') as pfile:\n",
    "            if i>0:\n",
    "                # skip header\n",
    "                pfile.readline()\n",
    "            content = content + pfile.read()\n",
    "            \n",
    "    # write\n",
    "    with open(dest_path, 'w') as dfile:\n",
    "        dfile.write(content)\n",
    "\n",
    "def hist_words(list_words):\n",
    "    \"\"\"Return a map: word->count from the list of words.\"\"\"\n",
    "    m = {}\n",
    "    for i, w in enumerate(list_words):\n",
    "        m[w] = 1 if w not in m else m[w] + 1\n",
    "    return m\n",
    "\n",
    "\n",
    "class CacheStemmer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.cache = {}\n",
    "        \n",
    "    def stem(self, word):\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        else:\n",
    "            st = self.stemmer.stem(word)\n",
    "            self.cache[word] = st\n",
    "            return st\n",
    "        \n",
    "        \n",
    "def stem_words(words, stemmer=CacheStemmer() ):\n",
    "    stem_words = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            s = stemmer.stem(w)\n",
    "            stem_words.append(s)\n",
    "        except UnicodeDecodeError as e:\n",
    "            #print('decode error for: %s'%w)\n",
    "            pass\n",
    "    return stem_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = CacheStemmer()\n",
    "stemmer.stem('recognize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fyear = 1988\n",
    "tyear = 2015\n",
    "stack_fname = 'Papers%d_%d.csv'%(fyear, tyear)\n",
    "stack_papers(range(fyear, tyear+1), stack_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Get the list of total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = CacheStemmer()\n",
    "set_words = set()\n",
    "paper_folders = ['../output%d'%y for y in range(fyear, tyear+1)]\n",
    "\n",
    "# dictionary compiled from Scowl spelling checking\n",
    "#dict_all = file_to_list('dict_all.txt')\n",
    "# dictionary from Wordnet\n",
    "dict_all = file_to_list('index.noun')\n",
    "#dict_all = file_to_list('index.verb')\n",
    "\n",
    "set_stop = set(file_to_list('stop_words.txt'))\n",
    "set_dict = set(dict_all)\n",
    "for oi, out_fol in enumerate(paper_folders):\n",
    "    fpath = os.path.join(out_fol, 'Papers.csv')\n",
    "    data = paper_dataframe(fpath)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        abstract = data['Abstract'][i]\n",
    "        content = data['PaperText'][i]\n",
    "        title = data['Title'][i]\n",
    "\n",
    "        title_words = tokenize(title)\n",
    "        abs_words = tokenize(abstract)\n",
    "        content_words = tokenize(content)\n",
    "        words = title_words + abs_words + content_words\n",
    "        # only include words (before stemming) that are in the dictionary\n",
    "        # and not in the list of stop words\n",
    "        words = [w for w in words if w in set_dict and w not in set_stop]\n",
    "        stwords = stem_words(words, stemmer)\n",
    "        set_words.update(stwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncond_words = uncond_filter_words(set_words)\n",
    "uncond_words.sort()\n",
    "list_to_file(uncond_words, 'uncond_filtered_words.txt')\n",
    "print('unconditionally filters words: %d'%(len(uncond_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2ind = dict(zip(uncond_words, range(len(uncond_words))) )\n",
    "data = paper_dataframe(stack_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_ind = []\n",
    "word_ind = []\n",
    "counts = []\n",
    "titles = []\n",
    "for i in range(data.shape[0]):\n",
    "    abstract = data['Abstract'][i]\n",
    "    content = data['PaperText'][i]\n",
    "    title = data['Title'][i]\n",
    "    titles.append(title)\n",
    "\n",
    "    title_words = tokenize(title)\n",
    "    abs_words = tokenize(abstract)\n",
    "    content_words = tokenize(content)\n",
    "    di_words = title_words + abs_words + content_words\n",
    "    stwords = stem_words(di_words, stemmer)\n",
    "    hist = hist_words(stwords)\n",
    "    for w, c in hist.iteritems():\n",
    "        if w in w2ind:\n",
    "            # ignore terms which are not in the vocabulary\n",
    "            wi = w2ind[w]\n",
    "            doc_ind.append(i)\n",
    "            word_ind.append(wi)\n",
    "            counts.append(c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save DT to a file \n",
    "DT = sp.csr_matrix( (counts, (doc_ind, word_ind)), shape=(data.shape[0], len(uncond_words)) )\n",
    "dt_fpath = 'DT_%d_%d.p'%(fyear, tyear)\n",
    "info = {'DT': DT, 'words': uncond_words, 'titles': titles}\n",
    "with open(dt_fpath, 'w') as f:\n",
    "    pickle.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Filter DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(dt_fpath, 'r') as f:\n",
    "    info = pickle.load(f)\n",
    "    \n",
    "words = info['words']\n",
    "DT = info['DT']\n",
    "titles = info['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document frequency of each word\n",
    "n = DT.shape[0]\n",
    "DF = np.array( (DT > 0).sum(0) )[0]\n",
    "df_lb = 5\n",
    "df_ub = int(0.4*n)\n",
    "\n",
    "print('#docs: %d'%n)\n",
    "print('original #words: %d'%len(words))\n",
    "print('#words with %d <= df: %d'% (df_lb, np.sum(DF>=df_lb) ) )\n",
    "print('#words with df <= %d: %d'% (df_ub, np.sum(DF<=df_ub) ) )\n",
    "df_I = np.logical_and(DF>=df_lb, DF<=df_ub)\n",
    "print('#words with %d <= df <= %d: %d'% \n",
    "      (df_lb, df_ub, np.sum( df_I) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(DF))\n",
    "plt.xlabel('word index')\n",
    "plt.ylabel('doc frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_words = np.array(words)[df_I]\n",
    "df_words.tolist()\n",
    "list_to_file(df_words, 'words_df%d_%d.txt'%(df_lb, df_ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_some_substring(text, substrings):\n",
    "    \"\"\"True if the text contains at least one substring in the list.\"\"\"\n",
    "    for i, s in enumerate(substrings):\n",
    "        if text.lower().find(s) > -1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stitles = sorted(titles)\n",
    "\n",
    "supervised_kws = ['large margin', 'classif', 'regression', 'kernel', 'ensemble', \n",
    "                  'neural net']\n",
    "deep_kws = ['deep', 'drop out', 'auto-encod', 'convolutional', 'neural net', 'belief net', \n",
    "           'boltzmann']\n",
    "neuro_kws = ['motor control', 'neural', 'neuron', 'spiking', 'spike', 'cortex', 'plasticity', \n",
    "            'neural decod', 'neural encod', 'brain imag', 'biolog', 'perception', 'cognitive', \n",
    "            'emotion', 'synap', 'neural population', 'cortical', 'firing rate', 'firing-rate', \n",
    "            'sensor']\n",
    "bayesian_kws = ['graphical model', 'bayesian', 'inference', 'mcmc', 'monte carlo', \n",
    "               'posterior', 'prior', 'variational', 'markov', 'latent', 'probabilistic', \n",
    "               'exponential family']\n",
    "kernel_kws = ['kernel', 'distribution embedding', 'support vector', 'gaussian process']\n",
    "learning_kws = ['learning theory', 'consistency', 'theoretical guarantee', \n",
    "                'complexity', 'pac-bayes', 'pac-learning', 'generalization', \n",
    "                'uniform converg', 'bound', 'deviation', 'inequality', 'risk min', 'minimax', \n",
    "               'structural risk', 'VC', 'rademacher', 'asymptotic']\n",
    "rl_kws = ['reinforce', 'regret', 'apprenticeship', 'game', 'TD', 'mdp', 'markov decision', \n",
    "         'agent', 'reward', 'player', 'thompson', 'policy', 'policies', 'value function', \n",
    "          'Q learning', 'Q-learning', 'planning', 'bandit', 'value iteration']\n",
    "\n",
    "neuro_titles = filter(lambda t: has_some_substring(t, neuro_kws), stitles)\n",
    "bayesian_titles = filter(lambda t: has_some_substring(t, bayesian_kws), stitles)\n",
    "deep_titles = [t for t in stitles if has_some_substring(t, deep_kws)]\n",
    "supervised_titles = [t for t in stitles if has_some_substring(t, supervised_kws)]\n",
    "kernel_titles = [t for t in stitles if has_some_substring(t, kernel_kws)]\n",
    "learning_titles = [t for t in stitles if has_some_substring(t, learning_kws)]\n",
    "rl_titles = [t for t in stitles if has_some_substring(t, rl_kws)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in deep_kws:\n",
    "    print '%s, '%w,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(learning_titles)\n",
    "learning_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_kws1 = bayesian_kws\n",
    "fname1 = 'bayes'\n",
    "title_kws2 = bayesian_kws\n",
    "fname2 = 'bayes'\n",
    "\n",
    "#title_kws1 = neuro_kws\n",
    "#fname1 = 'neuro'\n",
    "\n",
    "#title_kws2 = neuro_kws\n",
    "#fname2 = 'neuro'\n",
    "\n",
    "#title_kws1 = deep_kws\n",
    "#fname1 = 'deep'\n",
    "#title_kws2 = deep_kws\n",
    "#fname2 = 'deep'\n",
    "\n",
    "#title_kws2 = learning_kws\n",
    "#fname2 = 'learning'\n",
    "\n",
    "\"\"\"\n",
    "title_kws1 = bayesian_kws\n",
    "fname1 = 'bayes'\n",
    "\"\"\"\n",
    "#title_kws2 = deep_kws\n",
    "#fname2 = 'deep'\n",
    "\n",
    "np.random.seed(2990)\n",
    "set_ind1 = set([i for i in range(n) if has_some_substring(titles[i], title_kws1) ])\n",
    "set_ind2 = set([i for i in range(n) if has_some_substring(titles[i], title_kws2) ])    \n",
    "    \n",
    "#split_mode = 'disjoint'\n",
    "split_mode = 'random'\n",
    "\n",
    "if split_mode == 'disjoint':\n",
    "    # split the matched titles into two disjoint sets\n",
    "    common_ind = set_ind1 & set_ind2\n",
    "    # remove common documents in both\n",
    "    doc_I1 = np.array(list(set_ind1.difference(common_ind)))\n",
    "    doc_I2 = np.array(list(set_ind2.difference(common_ind)))\n",
    "    #doc_logI1 = np.zeros(n, dtype=np.bool)\n",
    "    #doc_logI1[doc_I1] = True\n",
    "    #doc_logI2 = np.zeros(n, dtype=np.bool)\n",
    "    #doc_logI2[doc_I2] = True\n",
    "elif split_mode == 'random':\n",
    "    # consider only fname1 and randomly split the samples into two disjoint halves.\n",
    "    list_ind1 = np.array(list(set_ind1))\n",
    "    half_ind = int(len(list_ind1)/2.001)\n",
    "    split_ind = np.random.choice(len(list_ind1), half_ind, replace=False)\n",
    "    doc_I1 = list_ind1[split_ind]\n",
    "    doc_I2 = np.array(list(set_ind1.difference(set(doc_I1))))\n",
    "\n",
    "else:\n",
    "    raise ValueError('unknown split_mode')\n",
    "Pdoc = DT[doc_I1, :]\n",
    "Qdoc = DT[doc_I2, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(doc_I1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('set 1. %d titles:'%len(doc_I1))\n",
    "for i in doc_I1:\n",
    "    print(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('set 2. %d titles:'%len(doc_I2))\n",
    "for i in doc_I2:\n",
    "    print(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove all the words that do not occur in any doc\n",
    "PQdoc = sp.vstack((Pdoc, Qdoc))\n",
    "pq_occur_wordI = np.array(PQdoc.sum(0) >= 1)[0]\n",
    "andI = np.logical_and(pq_occur_wordI, df_I)\n",
    "#andI = df_I\n",
    "pq_words = np.array(words)[andI]\n",
    "P = Pdoc[:, andI]\n",
    "Q = Qdoc[:, andI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('words left: %d'%P.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster as clu\n",
    "# k-means on the terms\n",
    "\n",
    "n_clusters = 2000\n",
    "clust = clu.KMeans(n_clusters=n_clusters, n_init=5, \n",
    "                   #init='random', \n",
    "                   random_state=12)\n",
    "PQ = sp.vstack((P,Q))\n",
    "PQ01 = PQ > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDF = (np.log(n) -  np.log(DF[andI]))\n",
    "P_tfidf = P.multiply(IDF)\n",
    "Q_tfidf = Q.multiply(IDF)\n",
    "tfidf = sp.vstack((P_tfidf, Q_tfidf))\n",
    "#row_norms = np.array(PQ.power(2).sum(1))**0.5\n",
    "#PQ_norm = PQ.multiply(1.0/row_norms)\n",
    "PQ_norm = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode = 'random'\n",
    "if mode=='random':\n",
    "    # Kacper: pick subset of random words\n",
    "    words_ind = np.random.choice(P_tfidf.shape[1], n_clusters, replace=False)\n",
    "elif mode=='kmeans':\n",
    "    clust.fit(PQ_norm.T)\n",
    "    mode_cluster = stats.mode(clust.labels_)[0][0]\n",
    "    mode_words = pq_words[clust.labels_==mode_cluster]\n",
    "    #for w in mode_words:\n",
    "    #    print w,\n",
    "    plt.plot(sorted(clust.labels_), 'o')\n",
    "    plt.xlabel('word index')\n",
    "    plt.ylabel('cluster index')\n",
    "    # first word in each cluster\n",
    "    words_ind = []\n",
    "    for ci in range(n_clusters):\n",
    "        ind = np.where(clust.labels_==ci)[0]\n",
    "        words_ind.append(ind[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_words = pq_words[words_ind]\n",
    "P_clus = P_tfidf[:, words_ind]\n",
    "Q_clus = Q_tfidf[:, words_ind]\n",
    "\n",
    "sortI = np.argsort(final_words)\n",
    "swords = final_words[sortI]\n",
    "P_sort = P_clus[:, sortI]\n",
    "Q_sort = Q_clus[:, sortI]\n",
    "\n",
    "# Construct the final PQ\n",
    "np_titles = np.array(titles)\n",
    "P_arr = np.array(P_sort)\n",
    "Q_arr = np.array(Q_sort)\n",
    "data = {'P': P_arr, 'Q': Q_arr, 'words': swords, \n",
    "        'P_titles': np_titles[doc_I1], 'Q_titles': np_titles[doc_I2], 'P_theme': fname1, \n",
    "       'Q_theme': fname2}\n",
    "\n",
    "fname = '%s_%s_np%d_nq%d_d%d.p'%(fname1, fname2, P_sort.shape[0], Q_sort.shape[0], n_clusters)\n",
    "with open(fname, 'w') as dest:\n",
    "    pickle.dump(data, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final word list\n",
    "for (i, w) in enumerate(swords):\n",
    "    if i%10==0:\n",
    "        print('')\n",
    "    print w, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
